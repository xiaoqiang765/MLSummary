{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c83a77",
   "metadata": {},
   "source": [
    "# vision in transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305a8d9",
   "metadata": {},
   "source": [
    "### 1、patch_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec2f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: xiao qiang\n",
    "Time: 2023/2/26 14:50 \n",
    "Version: env==torch py==3.9\n",
    "\"\"\"\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mmengine.model import BaseModule\n",
    "from mmengine.utils import to_2tuple\n",
    "from mmcv.cnn import build_conv_layer, build_activation_layer, build_norm_layer\n",
    "\n",
    "\n",
    "class AdaptivePadding(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies padding adaptively to the input\n",
    "    This module can make input get fully covered by filter you specified.It support two modes 'same' and 'corner'\n",
    "    the 'same' mode is same with 'SAME' padding mode in tensorflow, pad zero around input, the 'corner' mode would\n",
    "    pad zero to bottom right.\n",
    "    Args:\n",
    "        kernel_size(int|tuple): size of the kernel, default:1\n",
    "        stride(int|tuple): stride of the filter, default:1\n",
    "        dilation(int|tuple): spacing between kernel elements, default:1\n",
    "        padding(str): support 'same' and 'corner', 'corner' mode would pad zero to bottom right, and 'same' mode would\n",
    "            pad zero around input, default: 'corner'\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size=1, stride=1, dilation=1, padding='corner'):\n",
    "        super(AdaptivePadding, self).__init__()\n",
    "        assert padding in ('same', 'corner')\n",
    "        kernel_size = to_2tuple(kernel_size)\n",
    "        stride = to_2tuple(stride)\n",
    "        dilation = to_2tuple(dilation)\n",
    "        self.padding = padding\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    def get_pad_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "        calculate the padding size of input\n",
    "        Args:\n",
    "            input_shape: Arrange as (H, W)\n",
    "        Returns:\n",
    "            Tuple[int]: the padding size along the original H and W directions\n",
    "        \"\"\"\n",
    "        input_h, input_w = input_shape\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.stride\n",
    "        output_h = math.ceil(input_h / stride_h)\n",
    "        output_w = math.ceil(input_w / stride_w)\n",
    "        pad_h = max((output_h - 1) * stride_h +\n",
    "                    (kernel_h - 1) * self.dilation[0] + 1 - input_h, 0)\n",
    "        pad_w = max((output_w - 1) * stride_w +\n",
    "                    (kernel_w - 1) * self.dilation[1] + 1 - input_w, 0)\n",
    "        return pad_h, pad_w\n",
    "\n",
    "    def forward(self, x):\n",
    "        pad_h, pad_w = self.get_pad_shape(x.size()[-2:])\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            if self.padding == 'corner':\n",
    "                # F.pad(x, [left, right, top, bottom])\n",
    "                x = F.pad(x, [0, pad_w, 0, pad_h])\n",
    "            elif self.padding == 'same':\n",
    "                x = F.pad(x, [pad_w//2, pad_w-pad_w//2, pad_h//2, pad_h-pad_h//2])\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(BaseModule):\n",
    "    \"\"\"\n",
    "    Image to Patch embedding\n",
    "    use a conv layer to implement patch-embed.\n",
    "    Args:\n",
    "        in_channels(int): the number of input channels, defaults:3\n",
    "        embed_dims(int): the dimension of embedding, default:768\n",
    "        conv_type(str): the type of convolution to generate patch embedding, default:'conv2d'.\n",
    "        kernel_size(int): the kernel_size of embedding conv, default: 16\n",
    "        stride(int): the slide stride of embedding conv, default:16\n",
    "        padding(int|tuple|string): the padding length of embedding conv, when it is a string, it means the mode of\n",
    "            adaptive padding, support 'same' and 'corner' now, default:'corner'\n",
    "        dilation(int): the dilation rate of embedding conv, default: 1\n",
    "        bias(bool): bias of embed conv, default True\n",
    "        norm_cfg(dict, optional): config dict for normalization layer, default:None\n",
    "        input_size(int|tuple|None): the size of input, which will be used to calculate the out size, only works when\n",
    "            'dynamic_size' is False, default:None\n",
    "        init_cfg(dict, optional): the config for initialization, default:None\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, embed_dims=768, conv_type='Conv2d', kernel_size=16,\n",
    "                 stride=16, padding='corner', dilation=1, bias=True, norm_cfg=None, input_size=None, init_cfg=None):\n",
    "        super(PatchEmbed, self).__init__(init_cfg=init_cfg)\n",
    "        self.embed_dims = embed_dims\n",
    "        if stride is None:\n",
    "            stride = kernel_size\n",
    "        kernel_size = to_2tuple(kernel_size)\n",
    "        stride = to_2tuple(stride)\n",
    "        dilation = to_2tuple(dilation)\n",
    "        if isinstance(padding, str):\n",
    "            self.adaptive_padding = AdaptivePadding(\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                dilation=dilation,\n",
    "                padding=padding)\n",
    "            padding = 0\n",
    "        else:\n",
    "            self.adaptive_padding = 0\n",
    "            padding = to_2tuple(padding)\n",
    "        self.projection = build_conv_layer(dict(type=conv_type),\n",
    "                                           in_channels=in_channels,\n",
    "                                           out_channels=embed_dims,\n",
    "                                           kernel_size=kernel_size,\n",
    "                                           stride=stride,\n",
    "                                           padding=padding,\n",
    "                                           dilation=dilation,\n",
    "                                           bias=bias)\n",
    "        if norm_cfg is not None:\n",
    "            self.norm = build_norm_layer(norm_cfg, embed_dims)[1]\n",
    "        else:\n",
    "            self.norm = None\n",
    "        if input_size:\n",
    "            input_size = to_2tuple(input_size)\n",
    "            self.init_input_size = input_size\n",
    "            if self.adaptive_padding:\n",
    "                pad_h, pad_w = self.adaptive_padding.get_pad_shape(input_size)\n",
    "                input_h, input_w = input_size\n",
    "                input_h = input_h + pad_h\n",
    "                input_w = input_w + pad_w\n",
    "                input_size = (input_h, input_w)\n",
    "            # 卷积后输出尺寸计算：out_h = (h+2*padding-kernel_size)/stride+1\n",
    "            # 带有空洞卷积，有效卷积核大小为:new_kernel_size = kernel_size+(kernel_size-1)*(dilation-1)\n",
    "            # 将有效卷积核大小带入卷积输出尺寸计算即可\n",
    "            h_out = (input_size[0] + 2*padding[0]-dilation[0]*(kernel_size[0]-1)-1)//stride[0]+1\n",
    "            w_out = (input_size[1] + 2*padding[1]-dilation[1]*(kernel_size[1]-1)-1)//stride[1]+1\n",
    "            self.init_out_size = (h_out, w_out)\n",
    "        else:\n",
    "            self.init_input_size = None\n",
    "            self.init_out_size = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.adaptive_padding:\n",
    "            x = self.adaptive_padding(x)\n",
    "        x = self.projection(x)\n",
    "        out_size = (x.shape[2], x.shape[3])\n",
    "        # tensor.flatten(start_dim, end_dim)\n",
    "        # shape:[n, embed_dims, h, w] -> [n, embed_dims, h*w] -> [n, h*w, embed_dims] h*w表示token个数\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x, out_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17887120",
   "metadata": {},
   "source": [
    "### 2、attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9388852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: xiao qiang\n",
    "Time: 2023/2/22 08:31 \n",
    "Version: env==torch py==3.9\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mmengine.model import BaseModule\n",
    "from mmcv.cnn.bricks.drop import build_dropout\n",
    "from mmcls.models.utils.layer_scale import LayerScale\n",
    "\n",
    "\n",
    "class MultiheadAttention(BaseModule):\n",
    "    \"\"\"\n",
    "    Multi-head Attention Module.\n",
    "    This module implements multi-head attention that supports different input dims and embed dims. and it also supports\n",
    "    a shortcut from 'value', which is useful if input dims is not same with embed dims.\n",
    "    Args:\n",
    "        embed_dims(int): the embedding dimension.\n",
    "        num_heads(int): parallel attention heads.\n",
    "        input_dims(int, Optional): the input dimension, and if None, use 'embed_dims',defaults to None.\n",
    "        attn_drop(float): dropout rate of the dropout layer after the attention calculation of query and key, defaults 0\n",
    "        proj_drop(float): dropout rate of the dropout layer after the output projection.\n",
    "        dropout_layer(dict): the dropout config before adding the shortcut, defaults to dict(type='Dropout', drop_prob=0)\n",
    "        qkv_bias(bool): if True, add a learnable bias to q, k, v, defaults: True\n",
    "        qk_scale(float, optional): override default qk scale of 'head_dim**-0.5', if set, defaults None\n",
    "        proj_bias（bool）:if True, add a learnable bias to output projection, defaults to True.\n",
    "        v_shortcut（bool）:add a shortcut from value to output,it is usually used if input_dims is different from embed_dims\n",
    "        defaults:False\n",
    "        init_cfg(dict, optional):the config fro initialization,defaults to None\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dims, num_heads, input_dims=None, attn_drop=0., proj_drop=0.,\n",
    "                 dropout_layer=dict(type='Dropout', drop_prob=0.), qkv_bias=True, qk_scale=None, proj_bias=True,\n",
    "                 v_shortcut=False, use_layer_scale=False, init_cfg=None):\n",
    "        super(MultiheadAttention, self).__init__(init_cfg=init_cfg)\n",
    "        self.input_dims = input_dims or embed_dims\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_heads = num_heads\n",
    "        self.v_shortcut = v_shortcut\n",
    "        self.head_dims = embed_dims // num_heads\n",
    "        self.scale = qk_scale or self.head_dims**-0.5\n",
    "        self.qkv = nn.Linear(input_dims, embed_dims*3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(embed_dims, embed_dims, bias=proj_bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.out_drop = build_dropout(dropout_layer)\n",
    "        if use_layer_scale:\n",
    "            self.gamma1 = LayerScale(embed_dims)\n",
    "        else:\n",
    "            self.gamma1 = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # image shape: [B, N, patch_dim]\n",
    "        B, N, _ = x.shape\n",
    "        # input: [B, N, patch_dim], qkv后:[B, N, 3*embed_dims]\n",
    "        # qkv: 获得指定head的qkv矩阵，reshape qkv:[B, N, 3, num_heads, head_dims] -> [3, B, num_heads, N, head_dims]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dims).permute(2, 0, 3, 1, 4)\n",
    "        # q,k,v shape: [B, num_heads, N, head_dims]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        # q@k.transpose(-2, -1):[B, num_heads, N, head_dims]@[B, num_heads, head_dims, N] = [B, num_heads, N, N]\n",
    "        # 计算每张图片中每个head上q与k的attention, 后求softmax与dropout\n",
    "        attn = (q@k.transpose(-2, -1))*self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # [B, num_heads, N, N]@[B, num_heads, N, head_dims] = [B, num_heads, N, head_dims]->[B, N, num_heads, head_dims]\n",
    "        x = (attn@v).transpose(1, 2).reshape(B, N, self.embed_dims)\n",
    "        # 多头注意力机制出来后concat后再次进行线性映射，并且添加可学习的bias\n",
    "        x = self.proj(x)\n",
    "        # 对映射后的输出进行dropout，并进行gamma1缩放，后再进行drop_path正则化\n",
    "        # x:[B, N, embed_dims]\n",
    "        x = self.out_drop(self.gamma1(self.proj_drop(x)))\n",
    "        if self.v_shortcut:\n",
    "            x = v.squeeze(1)+x\n",
    "        return x\n",
    "\n",
    "\n",
    "def resize_pos_embed(pos_embed, src_shape, dst_shape, mode='bicubic', num_extra_tokens=1):\n",
    "    \"\"\"\n",
    "    Resize pos_embed weights.\n",
    "    Args:\n",
    "        pos_embed(torch.Tensor): Position embedding weights with shape [1, L, C].\n",
    "        src_shape(tuple): The resolution of down_sampled origin training image, in format(H, W).\n",
    "        dst_shape(tuple): The resolution of down_sampled new training image, in format(H, W)\n",
    "        mode(str): Algorithm used for up_sampling, choose one from 'nearest', 'linear','bilinear', 'bicubic' and\n",
    "            'trilinear'\n",
    "        num_extra_tokens(int): The number of extra tokens, such as cls_token, defaults to 1\n",
    "    \"\"\"\n",
    "    if src_shape[0] == dst_shape[0] and src_shape[1] == dst_shape[1]:\n",
    "        return pos_embed\n",
    "    assert pos_embed.ndim == 3, 'shape of pos_embed must be [1, L, C]'\n",
    "    _, L, C = pos_embed.shape\n",
    "    src_h, src_w = src_shape\n",
    "    assert L == src_h * src_w + num_extra_tokens, f'the length of \"pos_embed\" should equal src_h*src_w+extra_tokens'\n",
    "    extra_tokens = pos_embed[:, :num_extra_tokens]\n",
    "    src_weight = pos_embed[:, num_extra_tokens:]\n",
    "    # src_weight: [1, L-extra_tokens, C] -> [1, src_h*src_w, C] -> [1, C, src_h*src_w]\n",
    "    src_weight = src_weight.reshape(1, src_h, src_w, C).permute(0, 3, 1, 2)\n",
    "    dst_weight = F.interpolate(src_weight, size=dst_shape, align_corners=False, mode=mode)\n",
    "    # dst_weight: [1, C, dst_h*dst_w] - >[1, C, L] -> [1, L, C]\n",
    "    dst_weight = torch.flatten(dst_weight, 2).transpose(1, 2)\n",
    "    return torch.cat((extra_tokens, dst_weight), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c083c076",
   "metadata": {},
   "source": [
    "### 3、FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56786c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: xiao qiang\n",
    "Time: 2023/2/25 22:31 \n",
    "Version: env==torch py==3.9\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from mmengine.model import BaseModule\n",
    "from mmcv.cnn import build_conv_layer, build_activation_layer, build_norm_layer, Linear\n",
    "from mmcv.cnn.bricks.drop import build_dropout\n",
    "\n",
    "\n",
    "class FFN(BaseModule):\n",
    "    \"\"\"\n",
    "    Implements feed-forward networks with identity connection.\n",
    "    Args:\n",
    "        embed_dims(int): the feature dimension.Same as MultiHeadAttention, defaults:256\n",
    "        feedforward_channels(int): the hidden dimension of FFNs. defaults: 1024\n",
    "        num_fcs(int, optional): the number of fully_connected layers in FFNs, default:2\n",
    "        act_cfg(dict, optional): the activation config for FFNs, defaults: dict(type='ReLU')\n",
    "        ffn_drop(float, optional): Probability of an element to be zeroed in FFN, default:0.0\n",
    "        add_identity(bool, optional): whether to add the identity connection, default: True.\n",
    "        dropout_layer(obj:ConfigDict): the dropout_layer used when adding the shortcut.\n",
    "        init_cfg(obj: ConfigDict): the config for initialization, default:None\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dims=256, feedforward_channels=1024, num_fcs=2, act_cfg=dict(type='ReLU'),\n",
    "                 ffn_drop=0., dropout_layer=None, add_identity=True, init_cfg=None, **kwargs):\n",
    "        super(FFN, self).__init__(init_cfg=init_cfg)\n",
    "        assert num_fcs >= 2, f'num_fcs should be no less than 2, but got {num_fcs}'\n",
    "        self.embed_dims = embed_dims\n",
    "        self.feedforward_channels = feedforward_channels\n",
    "        self.num_fcs = num_fcs\n",
    "        self.act_cfg = act_cfg\n",
    "        self.activate = build_activation_layer(act_cfg)\n",
    "        layers = []\n",
    "        in_channels = embed_dims\n",
    "        for _ in range(num_fcs-1):\n",
    "            layers.append(nn.Sequential(Linear(in_channels, feedforward_channels), self.activate, nn.Dropout(ffn_drop)))\n",
    "            in_channels = feedforward_channels\n",
    "        layers.append(Linear(feedforward_channels, embed_dims))\n",
    "        layers.append(nn.Dropout(ffn_drop))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.dropout_layer = build_dropout(dropout_layer) if dropout_layer else torch.nn.Identity()\n",
    "        self.add_identity = add_identity\n",
    "\n",
    "    def forward(self, x, identity=None):\n",
    "        out = self.layers(x)\n",
    "        if not self.add_identity:\n",
    "            return self.dropout_layer(out)\n",
    "        if identity is None:\n",
    "            identity = x\n",
    "        return identity + self.dropout_layer(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226d93d",
   "metadata": {},
   "source": [
    "### 4、vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30bfd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: xiao qiang\n",
    "Time: 2023/2/21 22:37 \n",
    "Version: env==torch py==3.9\n",
    "\"\"\"\n",
    "from typing import Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from mmengine.model import BaseModule, ModuleList\n",
    "from mmcv.cnn import build_norm_layer\n",
    "from mmcls.models.utils import MultiheadAttention, to_2tuple, resize_pos_embed\n",
    "from mmcv.cnn.bricks.transformer import FFN\n",
    "from mmcls.models.backbones.base_backbone import BaseBackbone\n",
    "from mmengine.model.weight_init import trunc_normal_\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(BaseModule):\n",
    "    \"\"\"\n",
    "    Implement one encoder layer in vision transformer.\n",
    "    Args:\n",
    "        embed_dims（int）: the feature dimension.\n",
    "        num_heads（int）: parallel attention heads.\n",
    "        feedforward_channels（int）:the hidden dimension for FFNs.\n",
    "        drop_rate（float）:probability of element to be zeroed after the feed forward layer, defaults 0.\n",
    "        attn_drop_rate（float）: the drop out rate for attention output weights.\n",
    "        drop_path_rate（float）:stochastic depth rate, defaults to 0.\n",
    "        num_fcs（int）: the number of fully_connected layers for FFNs, defaults to 2.\n",
    "        qkv_bias（bool）:enable bias for qkv if True, defaults to True.\n",
    "        act_cfg（dict）: the activation config for FFNS, defaults to dict(type='GELU')\n",
    "        norm_cfg（dict）:config dict for normalization layer, defaults to dict(type='LN')\n",
    "        init_cfg（dict）: initialization config dict, defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dims, num_heads, feedforward_channels, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., num_fcs=2, qkv_bias=True, act_cfg=dict(type='GELU'), norm_cfg=dict(type='LN'),\n",
    "                 init_cfg=None):\n",
    "        super(TransformerEncoderLayer, self).__init__(init_cfg=init_cfg)\n",
    "        self.embed_dims = embed_dims\n",
    "        self.norm1_name, norm1 = build_norm_layer(norm_cfg, self.embed_dims, postfix=1)\n",
    "        self.add_module(self.norm1_name, norm1)\n",
    "        self.attn = MultiheadAttention(embed_dims=embed_dims, num_heads=num_heads, attn_drop=attn_drop_rate,\n",
    "                                       proj_drop=drop_rate, dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),\n",
    "                                       qkv_bias=qkv_bias)\n",
    "        self.norm2_name, norm2 = build_norm_layer(norm_cfg, self.embed_dims, postfix=2)\n",
    "        self.add_module(self.norm2_name, norm2)\n",
    "        self.ffn = FFN(embed_dims=embed_dims, feedforward_channels=feedforward_channels, num_fcs=num_fcs,\n",
    "                       ffn_drop=drop_rate, dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate, act_cfg=act_cfg))\n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return getattr(self, self.norm1_name)\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return getattr(self, self.norm2_name)\n",
    "\n",
    "    def init_weights(self):\n",
    "        super(TransformerEncoderLayer, self).init_weights()\n",
    "        for m in self.ffn.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init_xavier_uniform_(m.weight)\n",
    "                nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = self.ffn(self.norm2(x), identity=x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(BaseBackbone):\n",
    "    \"\"\"\n",
    "    Vision Transformer: A pytorch implement of 'an image is worth 16*16 words, transformers for image_recognition'\n",
    "    Args:\n",
    "        arch(str|dict): vision transformer architecture, if use string, choose from 'small','base','large','deit-tiny',\n",
    "            'deit-small' and 'deit-base', if use dict, it should have below keys:\n",
    "            - **embed_dims**(int): the dimension of embedding.\n",
    "            - **num_layers**(int): the number of transformer encoder layers.\n",
    "            - **num_heads**(int): the number of heads in attention modules.\n",
    "            - **feedforward_channels**(int): the hidden dimensions in feedforward modules\n",
    "            defaults: 'base'\n",
    "        img_size(int|tuple): the expected input image shape, because we support dynamic input shape, just set the argument\n",
    "            to the most common input image shape, defaults to 224.\n",
    "        patch_size(int|tuple): the patch size in patch embedding. defaults to 16.\n",
    "        in_channels(int): the number of input channels, defaults:3\n",
    "        out_indices(sequence|int): output from which stages, defaults to -1, means the last stage\n",
    "        drop_rate(float): probability of an element to be zeroed. defaults to 0\n",
    "        drop_path_rate(float): stochastic depth rate, defaults to 0.\n",
    "        qkv_bias(bool): whether to add bias for qkv in attention modules.\n",
    "        norm_cfg(dict): config dict for normalization layer, defaults to dict(type='LN')\n",
    "        final_norm(bool): whether to add a layer to normalize final feature map, defaults to True.\n",
    "        with_cls_token(bool): whether concatenating class token into image tokens as transformer input, defaults True\n",
    "        avg_token(bool): whether to use the mean patch token for classification, if true, the model will only\n",
    "            take the average of all patch tokens, defaults to False\n",
    "        frozen_stages(int): stages to be frozen, -1 means not freezing any parameters, defaults to -1\n",
    "        output_cls_token(bool): whether output cls token, if set true, with_cls_token must be true,defaults to True\n",
    "        beit_style(bool): whether to use beit_style, defaults to False\n",
    "        layer_scale_init_value(float): the initialization value for the learnable scaling of attention and FFN, default\n",
    "            to 0.1\n",
    "        interpolate_mode(str):select the interpolate mode for position embeding vector resize, defaults to 'bicubic'\n",
    "        patch_cfg(dict): configs of patch embeding, defaults to an empty dict\n",
    "        layer_cfgs(sequence|dict): configs of each transformer layer in encoder, defaults to an empty dict\n",
    "        init_cfg(dict, optional): initialization config dict, defaults to None\n",
    "    \"\"\"\n",
    "    arch_zoo = {\n",
    "        **dict.fromkeys(['s', 'small'], {'embed_dims': 768, 'num_layers': 8, 'num_heads': 8, 'feedforward_channels':768*3}),\n",
    "        **dict.fromkeys(['b', 'base'], {'embed_dims': 768, 'num_layers': 12, 'num_heads': 12, 'feedforward_channels': 3072}),\n",
    "        **dict.fromkeys(['l', 'large'], {'embed_dims': 1024, 'num_layers': 24, 'num_heads': 16, 'feedforward_channels': 4096}),\n",
    "        **dict.fromkeys(['h', 'huge'], {'embed_dims': 1280, 'num_layers': 32, 'num_heads': 16, 'feedforward_channels': 5120}),\n",
    "        **dict.fromkeys(['deit-t', 'deit-tiny'], {'embed_dims': 192, 'num_layers': 12, 'num_heads': 3, 'feedforward_channels': 192*4}),\n",
    "        **dict.fromkeys(['deit-s', 'deit-small'], {'embed_dims': 384, 'num_layers': 12, 'num_heads': 6, 'feedforward_channels': 384*4}),\n",
    "        **dict.fromkeys(['deit-b', 'deit-base'], {'embed_dims': 768, 'num_layers': 12, 'num_heads': 12, 'feedforward_channels': 768*4})\n",
    "    }\n",
    "    # some structures have multiple extra tokens, like Deit\n",
    "    num_extra_tokens = 1 # cls_token\n",
    "\n",
    "    def __init__(self, arch='base', img_size=224, patch_size=16, in_channels=3, out_indices=-1, drop_rate=0.,\n",
    "                 drop_path_rate=0., qkv_bias=True, norm_cfg=dict(type='LN'), final_norm=True, with_cls_token=True,\n",
    "                 avg_token=False, frozen_stages=-1, output_cls_token=True, beit_style=False, layer_scale_init_value=0.1,\n",
    "                 interpolate_mode='bicubic', patch_cfg=dict(), layer_cfgs=dict(), init_cfg=None):\n",
    "        super(VisionTransformer, self).__init__(init_cfg=init_cfg)\n",
    "        if isinstance(arch, str):\n",
    "            arch = arch.lower()\n",
    "            # set(dict) -> dict.keys\n",
    "            assert arch in set(self.arch_zoo), f'arch {arch} is not in default archs {set(self.arch_zoo)}'\n",
    "            self.arch_settings = self.arch_zoo[arch]\n",
    "        else:\n",
    "            essential_keys = {'embed_dims', 'num_layers', 'num_heads', 'feedforward_channels'}\n",
    "            assert isinstance(arch, dict) and essential_keys <= set(arch), f'custom arch needs a dict with keys ' \\\n",
    "                                                                           f'{essential_keys}'\n",
    "            self.arch_settings = arch\n",
    "        self.embed_dims = self.arch_settings['embed_dims']\n",
    "        self.num_layers = self.arch_settings['num_layers']\n",
    "        # img_size=224 -> (224, 224)\n",
    "        self.img_size = to_2tuple(img_size)\n",
    "        # set patch embedding\n",
    "        _patch_cfg = dict(\n",
    "            in_channels=in_channels,\n",
    "            input_size=img_size,\n",
    "            embed_dims=self.embed_dims,\n",
    "            conv_type='Conv2d',\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "        _patch_cfg.update(patch_cfg)\n",
    "        # execute an image to patch token\n",
    "        self.patch_embed = PatchEmbed(**_patch_cfg)\n",
    "        self.patch_resolution = self.patch_embed.init_out_size\n",
    "        num_patches = self.patch_resolution[0]*self.pathch_resolution[1]\n",
    "\n",
    "        # set cls token\n",
    "        if output_cls_token:\n",
    "            assert with_cls_token is True, f'with_cls_token must be True if set output_cls_token to True'\n",
    "        self.with_cls_token = with_cls_token\n",
    "        self.output_cls_token = output_cls_token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))\n",
    "        # set position embedding\n",
    "        self.interpolate_mode = interpolate_mode\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+self.num_extra_tokens, self.embed_dims))\n",
    "        self._register_load_state_dict_pre_hook(self._prepare_pos_embed)\n",
    "        self.drop_after_pos = nn.Dropout(p=drop_rate)\n",
    "        # set output indices\n",
    "        if isinstance(out_indices, int):\n",
    "            out_indices = [out_indices]\n",
    "        assert isinstance(out_indices, Sequence), f'out_indices must be sequence or int, but got {type(out_indices)}'\n",
    "        for i, index in enumerate(out_indices):\n",
    "            if index < 0:\n",
    "                out_indices[i] = self.num_layers + index\n",
    "            assert 0 <= out_indices[i] <= self.num_layers\n",
    "        self.out_indices = out_indices\n",
    "        # stochastic depth decay rule\n",
    "        dpr = np.linspace(0, drop_path_rate, self.num_layers)\n",
    "        self.layers = ModuleList()\n",
    "        if isinstance(layer_cfgs, dict):\n",
    "            layer_cfgs = [layer_cfgs]*self.num_layers\n",
    "        for i in range(self.num_layers):\n",
    "            _layer_cfg = dict(\n",
    "                embed_dims=self.embed_dims,\n",
    "                num_heads=self.arch_settings['num_heads'],\n",
    "                feedforward_channels=self.arch_settings['feedforward_channels'],\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=dpr[i],\n",
    "                qkv_bias=qkv_bias,\n",
    "                norm_cfg=norm_cfg)\n",
    "            _layer_cfg.update(layer_cfgs[i])\n",
    "            if beit_style:\n",
    "                pass\n",
    "            else:\n",
    "                self.layers.append(TransformerEncoderLayer(**_layer_cfg))\n",
    "        self.frozen_stages = frozen_stages\n",
    "        self.final_norm = final_norm\n",
    "        if final_norm:\n",
    "            self.norm1_name, norm1 = build_norm_layer(norm_cfg, self.embed_dims, postfix=1)\n",
    "            self.add_module(self.norm1_name, norm1)\n",
    "        self.avg_token = avg_token\n",
    "        if avg_token:\n",
    "            self.norm2_name, norm2 = build_norm_layer(norm_cfg, self.embed_dims, postfix=2)\n",
    "            self.add_module(self.norm2_name, norm2)\n",
    "        if self.frozen_stages > 0:\n",
    "            self._freeze_stages()\n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return getattr(self, self.norm1_name)\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return getattr(self, self.norm2_name)\n",
    "\n",
    "    def init_weights(self):\n",
    "        super(VisionTransformer, self).init_weights()\n",
    "        if not (isinstance(self.init_cfg, dict) and self.init_cfg['type'] == 'Pretrained'):\n",
    "            trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def _prepare_pos_embed(self, state_dict, prefix, *args, **kwargs):\n",
    "        name = prefix + 'pos_embed'\n",
    "        if name not in state_dict.keys():\n",
    "            return\n",
    "        ckpt_pos_embed_shape = state_dict[name].shape\n",
    "        if self.pos_embed.shape != ckpt_pos_embed_shape:\n",
    "            from mmengine.logging import MMLogger\n",
    "            logger = MMLogger.get_current_instance()\n",
    "            logger.info(f'resize the pos_embed shape from {ckpt_pos_embed_shape} to {self.pos_embed.shape}')\n",
    "            ckpt_pos_embed_shape = to_2tuple(int(np.sqrt(ckpt_pos_embed_shape.shape[1]-self.num_extra_tokensda)))\n",
    "            pos_embed_shape = self.patch_embed.init_out_size\n",
    "            state_dict[name] = resize_pos_embed(state_dict[name], ckpt_pos_embed_shape, pos_embed_shape,\n",
    "                                                self.interpolate_mode, self.num_extra_tokens)\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        # freeze order: pos_embed, pos_embed_dropout -> patch_embed -> cls_token -> layers -> if freeze last layers \\\n",
    "        # freeze the norm\n",
    "        # 涉及dropout, bn层的冻结时需要eval()模式\n",
    "        # freeze position embedding\n",
    "        self.pos_embed.requires_grad = False\n",
    "        # set dropout to eval mode\n",
    "        self.drop_after_pos.eval()\n",
    "        # freeze patch embedding\n",
    "        self.patch_embed.eval()\n",
    "        for param in self.patch_embed.parameters():\n",
    "            param.requires_grad = False\n",
    "        # freeze cls token\n",
    "        self.cls_token.requires_grad = False\n",
    "        # freeze layers\n",
    "        for i in range(1, self.frozen_stages+1):\n",
    "            m = self.layers[i-1]\n",
    "            m.eval()\n",
    "            for param in m.parameters():\n",
    "                param.requires_grad = False\n",
    "        # freeze the last layer norm\n",
    "        if self.frozen_stages == len(self.layers) and self.final_norm:\n",
    "            self.norm1.eval()\n",
    "            for param in self.norm1.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x, patch_resolution = self.patch_embed(x)\n",
    "        # cls_token: [1, 1, n_dim] -> [B, 1, n_dim]\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # pos_embed + patch_embed\n",
    "        x = x + resize_pos_embed(self.pos_embed, self.patch_resolution, patch_resolution, mode=self.interpolate_mode,\n",
    "                                 num_extra_tokens=self.nu_extra_tokens)\n",
    "        x = self.drop_after_pos(x)\n",
    "        if not self.with_cls_token:\n",
    "            x = x[:, 1:]\n",
    "        outs = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            if i == len(self.layers) - 1 and self.final_norm:\n",
    "                x = self.norm1(x)\n",
    "            if i in self.out_indices:\n",
    "                B, _, C = x.shape\n",
    "                if self.with_cls_token:\n",
    "                    # patch_token:[B, h, w, c] -> [B, c, h, w]\n",
    "                    patch_token = x[:, 1:].reshape(B, *patch_resolution, C)\n",
    "                    patch_token = patch_token.permute(0, 3, 1, 2)\n",
    "                    cls_token = x[:, 0]\n",
    "                else:\n",
    "                    patch_token = x.reshape(B, *patch_resolution, C)\n",
    "                    patch_token = patch_token.permute(0, 3, 1, 2)\n",
    "                    cls_token = None\n",
    "                if self.avg_token:\n",
    "                    # [B, h, w, c]\n",
    "                    patch_token = patch_token.permute(0, 2, 3, 1)\n",
    "                    patch_token = patch_token.reshape(B, patch_resolution[0]*patch_resolution[1], C).mean(dim=1)\n",
    "                    patch_token = self.norm2(patch_token)\n",
    "                if self.output_cls_token:\n",
    "                    out = [patch_token, cls_token]\n",
    "                else:\n",
    "                    out = patch_token\n",
    "                outs.append(out)\n",
    "        return tuple(outs)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_pos_embed(*args, **kwargs):\n",
    "        return resize_pos_embed(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015fb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmm2",
   "language": "python",
   "name": "openmm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
